{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unet_7layers_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPNFj/GyoPn6TEjxQuDxCWo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stebbibg/MSc_Fstudent_SLAM/blob/main/Unet_7layers_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZdnIZB_HqAe"
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EUymQ_nexFf",
        "outputId": "2f78d7bc-a710-43c7-80b6-1fb2bca41a36"
      },
      "source": [
        "from google.colab import drive\n",
        "import PIL\n",
        "from PIL import Image\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6CMnd7ZBn0P"
      },
      "source": [
        "from skimage.util import random_noise\n",
        "import random\n",
        "\n",
        "def getValImg(img, mask):\n",
        "  return img, mask\n",
        "\n",
        "def getTrainImg(img, mask):\n",
        "  p_crop = 0.9\n",
        "  p_affine = 0.2\n",
        "  p_color_jitter = 0.4\n",
        "  p_sp = 0.1\n",
        "  p_speckle = 0.1\n",
        "  p_erase = 0.2\n",
        "\n",
        "  # Blur parameters\n",
        "  p_gauss = 0.5\n",
        "  kernel_size = 15\n",
        "  \n",
        "  if random.random() < p_gauss:\n",
        "    kernel_size = random.randrange(5, 25, 2)\n",
        "    color_jitter_t = torchvision.transforms.Compose([\n",
        "      torchvision.transforms.GaussianBlur(kernel_size, sigma=(0.1, 3.0)),\n",
        "    ])\n",
        "    img = color_jitter_t(img)\n",
        "\n",
        "  if random.random() < p_color_jitter:\n",
        "    color_jitter_t = torchvision.transforms.Compose([\n",
        "      torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.01)\n",
        "    ])\n",
        "    img = color_jitter_t(img)\n",
        "\n",
        "  # random affine\n",
        "  if random.random() < p_affine:\n",
        "    affine_params = torchvision.transforms.RandomAffine.get_params((-8, 8), (0.05, 0.05), (0.95, 0.95), (-8, 8), img.size)\n",
        "    img, mask = TF.affine(img, *affine_params), TF.affine(mask, *affine_params)\n",
        "\n",
        "  # Random crop\n",
        "  if random.random() < p_crop:\n",
        "    new_width = random.randint(1088, 1554)\n",
        "    new_height = random.randint(1456, 2080)\n",
        "    resize = torchvision.transforms.Resize(size=(new_width, new_height), interpolation=PIL.Image.NEAREST)\n",
        "    img = resize(img)\n",
        "    mask = resize(mask)\n",
        "\n",
        "    i, j, h, w = torchvision.transforms.RandomCrop.get_params(\n",
        "        img, output_size=(1088, 1456))\n",
        "    img = TF.crop(img, i, j, h, w)\n",
        "    mask = TF.crop(mask, i, j, h, w)\n",
        "  \n",
        "  # speckle noise\n",
        "  if random.random() < p_speckle:\n",
        "    img_sp = np.asarray(img)\n",
        "    img_sp = random_noise(img_sp, mode='speckle', mean=0.1, seed=42,\n",
        "                                var=0.2)    \n",
        "    img_sp = img_sp.transpose((2, 0, 1))\n",
        "    img = torch.from_numpy(img_sp)\n",
        "    \n",
        "    img = torchvision.transforms.ToPILImage()(img).convert(\"RGB\")\n",
        "\n",
        "  # salt and pepper noise\n",
        "  if random.random() < p_sp:\n",
        "    img_sp = np.asarray(img)\n",
        "    img_sp = random_noise(img_sp, mode='s&p', salt_vs_pepper=0.5, clip=True)\n",
        "    img_sp = img_sp.transpose((2, 0, 1))\n",
        "\n",
        "    img = torch.from_numpy(img_sp)\n",
        "    \n",
        "    img = torchvision.transforms.ToPILImage()(img).convert(\"RGB\")\n",
        "\n",
        "  if random.random() < p_erase:\n",
        "    mask_tensor = TF.to_tensor(mask)\n",
        "    img = TF.to_tensor(img)\n",
        "    i, j, h, w, v = torchvision.transforms.RandomErasing.get_params(img, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=[0])\n",
        "\n",
        "    # Make it to three channels\n",
        "    mask_tensor = mask_tensor.repeat(3,1,1)\n",
        "\n",
        "    img = TF.erase(img, i, j, h, w, v)\n",
        "    mask_tensor = TF.erase(mask_tensor, i, j, h, w, v)\n",
        "    # Extract the first channel\n",
        "    mask, _, _ = mask_tensor.unbind(0)\n",
        "    mask = TF.to_pil_image(mask)\n",
        "    img = TF.to_pil_image(img)\n",
        "    \n",
        "  return img, mask\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ejkg6A5TItY"
      },
      "source": [
        "from os.path import splitext\n",
        "from os import listdir\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import torchvision\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import logging\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as TF\n",
        "import random\n",
        "\n",
        "#train_indx = []  # Indices for the training images\n",
        "#val_indx = []    # Indices for the validation images  \n",
        "\n",
        "class TrainingDataset(Dataset):\n",
        "    def __init__(self, imgs_dir, masks_dir, train_indx, val_indx, scale=1, mask_suffix=''):\n",
        "        self.train_indx = []\n",
        "        self.val_indx = []\n",
        "        self.imgs_dir = imgs_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.scale = scale\n",
        "        self.mask_suffix = mask_suffix\n",
        "        assert 0 < scale <= 1, 'Scale must be between 0 and 1'\n",
        "\n",
        "        self.ids = [splitext(file)[0] for file in listdir(imgs_dir)\n",
        "                    if not file.startswith('.')]\n",
        "        logging.info(f'Creating dataset with {len(self.ids)} examples')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    @classmethod\n",
        "    def preprocess(cls, pil_img, scale):\n",
        "        w, h = pil_img.size\n",
        "        newW, newH = int(scale * w), int(scale * h)\n",
        "        assert newW > 0 and newH > 0, 'Scale is too small'\n",
        "        pil_img = pil_img.resize((newW, newH))\n",
        "\n",
        "        img_nd = np.array(pil_img)\n",
        "\n",
        "        if len(img_nd.shape) == 2:\n",
        "            img_nd = np.expand_dims(img_nd, axis=2)\n",
        "\n",
        "        # HWC to CHW\n",
        "        img_trans = img_nd.transpose((2, 0, 1))\n",
        "\n",
        "        return img_trans\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        idx = self.ids[i]\n",
        "        mask_file = glob(self.masks_dir + idx + self.mask_suffix + '.*')\n",
        "        img_file = glob(self.imgs_dir + idx + '.*')\n",
        "\n",
        "        mask = Image.open(mask_file[0])\n",
        "        img = Image.open(img_file[0])\n",
        "\n",
        "        if int(i) in self.train_indx:\n",
        "          img, mask = getTrainImg(img, mask)\n",
        "        else:\n",
        "          img, mask = getValImg(img, mask)\n",
        "        \n",
        "        img = self.preprocess(img, self.scale)\n",
        "        mask = self.preprocess(mask, self.scale)\n",
        "\n",
        "        img_tensor = torch.from_numpy(img).type(torch.FloatTensor)\n",
        "\n",
        "        preprocess_image = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "        img_tensor = preprocess_image(img_tensor)\n",
        "\n",
        "        return {\n",
        "            'image': img_tensor,\n",
        "            'mask': torch.from_numpy(mask).type(torch.FloatTensor)\n",
        "        }"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck5qIxCVUkSO"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        dy = x2.size()[2] - x1.size()[2]\n",
        "        dx = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [dx // 2, dx - dx // 2,\n",
        "                        dy // 2, dy - dy // 2])\n",
        "        \n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff9Z0a3fUmKP"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        factor = 2\n",
        "\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512 // factor)\n",
        "        self.up2 = Up(512, 256 // factor)\n",
        "        self.up3 = Up(256, 128 // factor)\n",
        "        self.up4 = Up(128, 64)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x = self.up2(x4, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        outp = self.outc(x)\n",
        "        return outp\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2NBUpj6AbNJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfb1d0ca-159f-4b84-f4ee-572b426c7df8"
      },
      "source": [
        "n = UNet(n_channels=3, n_classes=5)\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(count_parameters(n))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4284613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d4xUy8QUBVN"
      },
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "model = UNet(n_channels=3, n_classes=5)\n",
        "device = torch.device('cuda:0')\n",
        "model.to(device)\n",
        "\n",
        "batch_size = 2\n",
        "img_scale = 0.5\n",
        "dir_img = '/content/drive/My Drive/Colab Notebooks/imgs/'\n",
        "dir_mask = '/content/drive/My Drive/Colab Notebooks/masks/'\n",
        "lr = 0.0001\n",
        "dir_checkpoint = '/content/drive/My Drive/Colab Notebooks//checkpoints/'\n",
        "dataset = TrainingDataset(dir_img, dir_mask,[], [], img_scale)\n",
        "\n",
        "n_val = 133\n",
        "n_test = 100\n",
        "n_train = 850\n",
        "train, val, test = random_split(dataset, [n_train, n_val, n_test])\n",
        "dataset.train_indx = train.indices\n",
        "dataset.val_indx = val.indices\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
        "val_loader = DataLoader(val, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)\n",
        "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 200\n",
        "\n",
        "# Load a previously saved model\n",
        "#model.load_state_dict(torch.load('/content/drive/My Drive/Colab Notebooks//checkpoints/8.pth'))\n",
        "\n",
        "for i in range(epochs):\n",
        "  current_epoch_loss = 0\n",
        "  model.train()\n",
        "  ctr = 0\n",
        "  for batch in train_loader:\n",
        "    ctr += 1\n",
        "    optimizer.zero_grad()\n",
        "    imgs = batch['image']\n",
        "    true_masks = batch['mask']\n",
        "    imgs = imgs.to(device=device, dtype=torch.float32)\n",
        "    mask_type = torch.float32\n",
        "    true_masks = true_masks.to(device=device, dtype=torch.long)\n",
        "    masks_pred = model(imgs)\n",
        "\n",
        "    true_masks_flat = torch.squeeze(true_masks, dim=0)\n",
        "    true_masks_flat = true_masks_flat.squeeze(1)\n",
        "\n",
        "    loss = criterion(masks_pred, true_masks_flat)\n",
        "\n",
        "    current_epoch_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  model.eval()\n",
        "  val_loss = 0\n",
        "  for batch in val_loader:\n",
        "    imgs = batch['image']\n",
        "    true_masks = batch['mask']\n",
        "    imgs = imgs.to(device=device, dtype=torch.float32)\n",
        "    mask_type = torch.float32\n",
        "    true_masks = true_masks.to(device=device, dtype=torch.long)\n",
        "    masks_pred = model(imgs)\n",
        "\n",
        "    true_masks_flat = torch.squeeze(true_masks, dim=1)\n",
        "    true_masks_flat = true_masks_flat.squeeze(1)\n",
        "\n",
        "    loss = criterion(masks_pred, true_masks_flat)\n",
        "    val_loss += loss.item()\n",
        "  total_iou = 0\n",
        "  model.eval()\n",
        "  test_counter = 0\n",
        "  for batch in test_loader:\n",
        "    imgs = batch['image']\n",
        "    true_masks = batch['mask']\n",
        "    imgs = imgs.to(device=device, dtype=torch.float32)\n",
        "    mask_type = torch.float32\n",
        "    true_masks = true_masks.to(device=device, dtype=torch.long)\n",
        "    masks_pred = model(imgs)\n",
        "\n",
        "    masks_pred = torch.argmax(masks_pred, axis=1)\n",
        "\n",
        "    true_masks_flat = torch.squeeze(true_masks, dim=1)\n",
        "\n",
        "    true_masks_flat = true_masks_flat.cpu().numpy().reshape(-1)\n",
        "    masks_pred = masks_pred.cpu().numpy().reshape(-1)\n",
        "\n",
        "    iou = jaccard_score(true_masks_flat, masks_pred, labels=[1, 2, 3, 4], average= 'micro')\n",
        "    total_iou += iou\n",
        "    test_counter += 1\n",
        "  \n",
        "  iou = total_iou /test_counter\n",
        "  current_epoch_loss /= n_train\n",
        "  val_loss /= n_val\n",
        "  print(\"epoch: \" + str(i + 1) + \" training loss: \" + str(current_epoch_loss) + \" val loss: \" + str(val_loss) + \" iou: \" + str(iou) + \"\\n\")\n",
        "  file1 = open(\"/content/drive/My Drive/Colab Notebooks/training_colab.txt\", \"a\")\n",
        "  file1.write(str(current_epoch_loss) + \" \" + str(val_loss) + \" \" + str(iou) + \"\\n\")\n",
        "  torch.save(copy.deepcopy(model.state_dict()), dir_checkpoint + str(i) + \".pth\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt1sNORhu-y-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6yB9oAGW-tl"
      },
      "source": [
        "# New Section\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ]
    }
  ]
}